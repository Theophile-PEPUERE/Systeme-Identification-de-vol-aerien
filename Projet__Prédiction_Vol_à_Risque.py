# -*- coding: utf-8 -*-
"""TPE_Bag of word.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DxmJhysi0PLyWRBnZcASVj7H0wFXk6pH

**1 - Extraction des titres**
"""

!pip install PyPDF2

!apt-get install python3-pypdf2

import os
import PyPDF2
import csv

# Chemin d'accès vers le dossier contenant les fichiers PDF
pdf_folder_path = r"/content/drive/MyDrive/TPE_Mon_Travail/Rapport_BEA"

# Chemin d'accès vers le fichier CSV de sortie
csv_output_path = r"/content/drive/MyDrive/TPE_Mon_Travail/dataset_title.csv"

# Créer un fichier CSV pour stocker les données
with open(csv_output_path, 'w', newline='', encoding='utf-8') as csv_file:
    csv_writer = csv.writer(csv_file)
    csv_writer.writerow(['id', 'pdf_title'])  # Écrire l'en-tête du CSV

    # Liste des noms de fichiers PDF
    pdf_file_names = os.listdir(pdf_folder_path)

    # Parcourir chaque fichier PDF
    for idx, pdf_file_name in enumerate(pdf_file_names, start=1):  # Commencer à partir de 1
        pdf_file_path = os.path.join(pdf_folder_path, pdf_file_name)

        # Vérifier si c'est un fichier (et non un répertoire)
        if os.path.isfile(pdf_file_path) and pdf_file_path.lower().endswith('.pdf'):
            # Supprimer le texte "Conséquences et dommages" du nom du fichier
            pdf_file_name = pdf_file_name.replace("Conséquences et dommages", "").strip()

            # Utiliser PyPDF2 pour extraire le texte du PDF
            with open(pdf_file_path, 'rb') as pdf_file:
                pdf_reader = PyPDF2.PdfReader(pdf_file)
                pdf_text = ""
                for page_num in range(len(pdf_reader.pages)):
                    pdf_text += pdf_reader.pages[page_num].extract_text()

                # Extraire la valeur de l'attribut "Conséquences et dommages"
                start_idx = pdf_text.find("Conséquences et dommages")
                end_idx = pdf_text.find("\n", start_idx)
                consequence_value = pdf_text[start_idx:end_idx].strip()

                # Écrire les données dans le fichier CSV
                csv_writer.writerow([idx, pdf_file_name, consequence_value])

print("Extraction et enregistrement dans le fichier CSV terminés.")

"""**2 - Nettoyage des titres extraits**"""

import pandas as pd
import re

# Chemin vers le fichier CSV
csv_file_path = r"/content/drive/MyDrive/TPE_Mon_Travail/dataset_title.csv"

# Lire le fichier CSV en utilisant pandas
data = pd.read_csv(csv_file_path, encoding='utf-8')

# Supprimer l'expression "Conséquences et dommages" de la colonne "pdf_title"
data['pdf_title'] = data['pdf_title'].str.replace('Conséquences et dommages', '')

# Supprimer l'expression "Sauf précision" de la colonne "pdf_title"
data['pdf_title'] = data['pdf_title'].str.replace('Sauf précision', '')

# Liste des articles à supprimer
articles_a_supprimer = ["le", "la", "les", "de", "du", "des", "un", "une", "au", "avant", "à", "et", "en", "dans", "que", "qui", "qu'", "autre", "dont", "sur", "il", "nous", "ce", "ces", "se", "donc", "alors"]

# Fonction pour nettoyer le texte en supprimant les articles, la ponctuation et les caractères spéciaux
def nettoyer_texte(texte):
    # Convertir la valeur en chaîne de caractères
    texte = str(texte)

    # Supprimer la ponctuation et les caractères spéciaux avec une expression régulière
    texte = re.sub(r'[^\w\s]', '', texte)

    # Gérer le cas de "d étruit" spécifiquement
    texte = re.sub(r'\bd\sétruit\b', 'détruit', texte, flags=re.IGNORECASE)

    # Utiliser une expression régulière pour séparer les mots
    mots = re.findall(r'\b\w+\b', texte)

    # Filtrer les mots en supprimant les articles et autres mots spécifiques
    mots_filtrés = [mot for mot in mots if mot.lower() not in articles_a_supprimer]

    # Rejoindre les mots filtrés pour former le texte nettoyé
    texte_nettoyé = ' '.join(mots_filtrés)

    # Remplacer les mots spécifiques
    mots_a_remplacer = ["ULM", "paramoteur", "hélicoptère", "aéronef", "planeur", "Planeur"]
    for mot in mots_a_remplacer:
        texte_nettoyé = texte_nettoyé.replace(mot, 'avion')

    return texte_nettoyé

# Appliquer la fonction de nettoyage à la colonne "pdf_title"
data['pdf_title'] = data['pdf_title'].apply(nettoyer_texte)

# Supprimer les caractères numériques
data['pdf_title'] = data['pdf_title'].apply(lambda x: re.sub(r'\d', '', x))

# Enregistrer le résultat dans un nouveau fichier CSV
cleaned_csv_file_path = r"/content/drive/MyDrive/TPE_Mon_Travail/BD1_Title.csv"
data.to_csv(cleaned_csv_file_path, index=False)

# Afficher les premières lignes du DataFrame nettoyé
print(data)

"""**3 - Lemmatisation** **du** **contenu** **pdf_title**"""

import spacy
import pandas as pd

# Charger le modèle téléchargé
nlp = spacy.load("fr_core_news_lg")

# Chemin d'entrée et de sortie des fichiers CSV
chemin_entree = r"/content/drive/MyDrive/TPE_Mon_Travail/BD1_Title.csv"
chemin_sortie = r"/content/drive/MyDrive/TPE_Mon_Travail/BD1_Title_Lemma.csv"

# Lire le fichier CSV en utilisant pandas avec un encodage spécifié
data = pd.read_csv(chemin_entree, encoding="utf-8")

# Créer une liste pour stocker les résultats
resultats = []

# Appliquer la transformation sur chaque ligne du fichier
for index, row in data.iterrows():
    # Vérifier le type de la valeur dans la colonne "pdf_title"
    if isinstance(row["pdf_title"], str):
        # Appliquer le modèle de langue sur le texte de chaque ligne
        doc = nlp(row["pdf_title"])

        # Lemmatisation des mots dans le texte
        lemmes = [token.lemma_ for token in doc]

        # Joindre les lemmes pour former un texte lemmatisé
        texte_lemmatise = " ".join(lemmes)

        # Ajouter le texte lemmatisé aux résultats
        resultats.append([row["id"], texte_lemmatise])  # Stocke l'id et le texte lemmatisé
    else:
        # Handle other types if needed
        resultats.append([row["id"], ""])

# Créer un DataFrame à partir des résultats
df_resultats = pd.DataFrame(resultats, columns=["id", "pdf_title_lemmatized"])  # Deux colonnes

# Sauvegarder le DataFrame dans le fichier de sortie avec un encodage spécifié
df_resultats.to_csv(chemin_sortie, index=False, encoding="utf-8")

# Afficher le contenu du fichier de sortie
print(df_resultats)



"""**Pourquoi Lemmatiser un contenu de caractère ?**

La lemmatisation est un processus important dans le traitement du langage naturel (NLP) qui vise à réduire les mots à leur forme de base ou lemmes. La lemmatisation est effectuée pour plusieurs raisons importantes :

**Normalisation :** La lemmatisation contribue à normaliser les mots, en réduisant les mots fléchis ou conjugés à leur forme de base. Par exemple, les mots "courir", "court", "courant", "courait" sont tous réduits à leur lemme "courir". Cela simplifie la comparaison de mots et améliore la précision des analyses textuelles.

**Réduction de la dimension :** La lemmatisation réduit la dimension du vocabulaire en remplaçant les formes fléchies ou dérivées par leurs lemmes correspondants. Cela permet de travailler avec un ensemble de mots plus restreint, ce qui est utile pour la classification, la regroupement (clustering) et d'autres tâches de NLP.

**Amélioration de la recherche d'information :** La lemmatisation facilite la recherche de documents pertinents. En réduisant les mots à leur forme de base, la lemmatisation permet d'inclure les formes fléchies et les lemmes dans une recherche, ce qui augmente les chances de trouver des informations pertinentes.

**Traitement de la flexion verbale et nominale :** En français, comme dans d'autres langues, les verbes et les noms peuvent prendre de nombreuses formes fléchies. La lemmatisation aide à gérer ces variations, ce qui est particulièrement utile dans le cadre de l'analyse de texte, de la recherche d'information et de la classification.

**Amélioration de la cohérence :** La lemmatisation améliore la cohérence en normalisant le texte, ce qui facilite la comparaison de textes et la détection de similitudes.

Donc, la lemmatisation est un processus essentiel dans le traitement du langage naturel qui permet de simplifier, normaliser et améliorer la qualité de l'analyse des textes. Elle est particulièrement utile pour rendre les données textuelles plus faciles à traiter, à comparer et à analyser.

**4 - Bag ofword**
"""

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

# Charger le fichier CSV
data = pd.read_csv(r"/content/drive/MyDrive/TPE_Mon_Travail/BD1_Title_Lemma.csv")  # Assurez-vous que le chemin est correct

# Remplacer les NaN par des chaînes vides dans la colonne "pdf_title_lemmatized"
data["pdf_title_lemmatized"] = data["pdf_title_lemmatized"].fillna("")

# Récupérer les textes à partir du DataFrame
textes = data["pdf_title_lemmatized"]

# Créer une instance du vectoriseur BoW
vectorizer = CountVectorizer()

# Adapter le vectoriseur aux textes et transformer les textes en vecteurs BoW
bow_matrix = vectorizer.fit_transform(textes)

# Créer un DataFrame pour les vecteurs BoW
bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())

# Ajouter la colonne "id" du DataFrame d'origine en première colonne
bow_df.insert(0, "id", data["id"])

# Enregistrer le DataFrame dans un fichier CSV
bow_df.to_csv(r"/content/drive/MyDrive/TPE_Mon_Travail/BD1-BOW_Title.csv", index=False)

print(bow_df)

"""**Clustering sur le Bag of word**

**Pour K = 3**
"""

pip install scikit-learn

import pandas as pd
from sklearn.cluster import KMeans
import numpy as np

# Charger le fichier CSV contenant les vecteurs BoW
data = pd.read_csv(r"/content/drive/MyDrive/TPE_Mon_Travail/BD1-BOW_Title.csv")

# Récupérer la colonne "id"
ids = data["id"]

# Supprimer la colonne "id" du DataFrame
data.drop("id", axis=1, inplace=True)

# Spécifier le nombre de clusters (vous pouvez ajuster ce nombre)
nombre_de_clusters = 3

# Créer une instance du modèle K-Means
kmeans = KMeans(n_clusters=nombre_de_clusters, random_state=0)

# Adapter le modèle aux données
kmeans.fit(data)

# Ajouter les étiquettes de cluster au DataFrame original
data["cluster"] = kmeans.labels_

# Réinsérer la colonne "id" en première colonne
data.insert(0, "id", ids)

# Enregistrer le DataFrame avec les étiquettes de cluster dans un fichier CSV
data.to_csv(r"/content/drive/MyDrive/TPE_Mon_Travail/BD1-Cluster3_Title.csv", index=False)

# Afficher les résultats
print(data)

"""**Représentation graphiques**

**Pour K = 3**
"""

import matplotlib.pyplot as plt

# Charger le fichier CSV contenant les vecteurs BoW et les étiquettes de cluster
data = pd.read_csv("/content/drive/MyDrive/TPE_Mon_Travail/BD1-Cluster3_Title.csv")

# Récupérer la colonne "id"
ids = data["id"]

# Supprimer la colonne "id" du DataFrame
data.drop("id", axis=1, inplace=True)

# Spécifier le nombre de clusters (vous pouvez ajuster ce nombre)
nombre_de_clusters = 3

# Créer une instance du modèle K-Means
kmeans = KMeans(n_clusters=nombre_de_clusters, random_state=0)

# Adapter le modèle aux données
kmeans.fit(data)

# Ajouter les étiquettes de cluster au DataFrame original
data["cluster"] = kmeans.labels_

# Réduire les données en 2 dimensions (ou tout autre nombre de dimensions)
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
data_pca = pca.fit_transform(data.drop("cluster", axis=1))

# Ajouter les étiquettes de cluster à la version en 2D des données
data_pca = pd.DataFrame(data_pca, columns=["Dimension_1", "Dimension_2"])
data_pca["cluster"] = data["cluster"]

# Créer un nuage de points coloré pour les clusters
plt.figure(figsize=(8, 6))
colors = ['r', 'g', 'b']  # Couleurs pour chaque cluster
for cluster in range(nombre_de_clusters):
    plt.scatter(data_pca[data_pca['cluster'] == cluster]['Dimension_1'],
                data_pca[data_pca['cluster'] == cluster]['Dimension_2'],
                label=f'Cluster {cluster}', c=colors[cluster], s=50)

plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.title('Nuage de points des clusters (K=3)')
plt.legend()
plt.show()

import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Charger le fichier CSV contenant les vecteurs BoW
data = pd.read_csv("/content/drive/MyDrive/TPE_Mon_Travail/BD1-BOW_Title.csv")

# Récupérer la colonne "id"
ids = data["id"]

# Supprimer la colonne "id" du DataFrame
data.drop("id", axis=1, inplace=True)

# Spécifier le nombre de clusters (vous pouvez ajuster ce nombre)
nombre_de_clusters = 3

# Créer une instance du modèle K-Means
kmeans = KMeans(n_clusters=nombre_de_clusters, random_state=0)

# Adapter le modèle aux données
kmeans.fit(data)

# Ajouter les étiquettes de cluster au DataFrame original
data["cluster"] = kmeans.labels_

# Créer un histogramme des clusters
plt.hist(data["cluster"], bins=range(nombre_de_clusters + 1), align="left", rwidth=0.8)
plt.xticks(range(nombre_de_clusters))
plt.xlabel("Cluster")
plt.ylabel("Nombre de données")
plt.title("Répartition des données par cluster (K=3)")
plt.show()

"""**Cluster** **Pour K = 4**"""

import pandas as pd
from sklearn.cluster import KMeans
import numpy as np

# Charger le fichier CSV contenant les vecteurs BoW
data = pd.read_csv(r"/content/drive/MyDrive/TPE_Mon_Travail/BD1-BOW_Title.csv")

# Récupérer la colonne "id"
ids = data["id"]

# Supprimer la colonne "id" du DataFrame
data.drop("id", axis=1, inplace=True)

# Spécifier le nombre de clusters (vous pouvez ajuster ce nombre)
nombre_de_clusters = 4

# Créer une instance du modèle K-Means
kmeans = KMeans(n_clusters=nombre_de_clusters, random_state=0)

# Adapter le modèle aux données
kmeans.fit(data)

# Ajouter les étiquettes de cluster au DataFrame original
data["cluster"] = kmeans.labels_

# Réinsérer la colonne "id" en première colonne
data.insert(0, "id", ids)

# Enregistrer le DataFrame avec les étiquettes de cluster dans un fichier CSV
data.to_csv(r"/content/drive/MyDrive/TPE_Mon_Travail/BD1-Cluster4_Title.csv", index=False)

# Afficher les résultats
print(data)

"""**Représentation graphiques**

**Pour K = 4**
"""

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Charger le fichier CSV contenant les vecteurs BoW
data = pd.read_csv("/content/drive/MyDrive/TPE_Mon_Travail/BD1-Cluster4_Title.csv")

# Récupérer la colonne "id"
ids = data["id"]

# Supprimer la colonne "id" du DataFrame
data.drop("id", axis=1, inplace=True)

# Spécifier le nombre de clusters (vous pouvez ajuster ce nombre)
nombre_de_clusters = 4

# Créer une instance du modèle K-Means
kmeans = KMeans(n_clusters=nombre_de_clusters, random_state=0)

# Adapter le modèle aux données
kmeans.fit(data)

# Ajouter les étiquettes de cluster au DataFrame original
data["cluster"] = kmeans.labels_

# Réduire les dimensions des données à 2 composantes avec PCA
pca = PCA(n_components=2)
data_pca = pca.fit_transform(data.drop("cluster", axis=1))

# Ajouter les étiquettes de cluster à la version en 2D des données
data_pca = pd.DataFrame(data_pca, columns=["Dimension_1", "Dimension_2"])
data_pca["cluster"] = data["cluster"]

# Créer un nuage de points avec des couleurs pour chaque cluster
plt.figure(figsize=(10, 6))
colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']
for i in range(nombre_de_clusters):
    cluster_data = data_pca[data_pca['cluster'] == i]
    plt.scatter(cluster_data['Dimension_1'], cluster_data['Dimension_2'], c=colors[i], label=f'Cluster {i}')

plt.xlabel("Dimension_1")
plt.ylabel("Dimension_2")
plt.title("Représentation graphique des clusters (K=4)")
plt.legend()
plt.show()

import pandas as pd
from sklearn.cluster import KMeans
import numpy as np
import matplotlib.pyplot as plt

# Charger le fichier CSV contenant les vecteurs BoW
data = pd.read_csv("/content/drive/MyDrive/TPE_Mon_Travail/BD1-BOW_Title.csv")

# Récupérer la colonne "id"
ids = data["id"]

# Supprimer la colonne "id" du DataFrame
data.drop("id", axis=1, inplace=True)

# Spécifier le nombre de clusters (vous pouvez ajuster ce nombre)
nombre_de_clusters = 4

# Créer une instance du modèle K-Means
kmeans = KMeans(n_clusters=nombre_de_clusters, random_state=0)

# Adapter le modèle aux données
kmeans.fit(data)

# Ajouter les étiquettes de cluster au DataFrame original
data["cluster"] = kmeans.labels_

# Créer un histogramme des clusters
plt.hist(data["cluster"], bins=range(0, nombre_de_clusters + 1), align='left', rwidth=0.8)
plt.xlabel('Cluster')
plt.ylabel('Nombre de points')
plt.title('Histogramme des clusters (K=4)')
plt.xticks(range(0, nombre_de_clusters))
plt.show()

"""**Cluster** **Pour K = 5**"""

import pandas as pd
from sklearn.cluster import KMeans
import numpy as np

# Charger le fichier CSV contenant les vecteurs BoW
data = pd.read_csv(r"/content/drive/MyDrive/TPE_Mon_Travail/BD1-BOW_Title.csv")

# Récupérer la colonne "id"
ids = data["id"]

# Supprimer la colonne "id" du DataFrame
data.drop("id", axis=1, inplace=True)

# Spécifier le nombre de clusters (vous pouvez ajuster ce nombre)
nombre_de_clusters = 5

# Créer une instance du modèle K-Means
kmeans = KMeans(n_clusters=nombre_de_clusters, random_state=0)

# Adapter le modèle aux données
kmeans.fit(data)

# Ajouter les étiquettes de cluster au DataFrame original
data["cluster"] = kmeans.labels_

# Réinsérer la colonne "id" en première colonne
data.insert(0, "id", ids)

# Enregistrer le DataFrame avec les étiquettes de cluster dans un fichier CSV
data.to_csv(r"/content/drive/MyDrive/TPE_Mon_Travail/BD1-Cluster5_Title.csv", index=False)

# Afficher les résultats
print(data)

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Charger le fichier CSV contenant les vecteurs BoW
data = pd.read_csv("/content/drive/MyDrive/TPE_Mon_Travail/BD1-Cluster5_Title.csv")

# Récupérer la colonne "id"
ids = data["id"]

# Supprimer la colonne "id" du DataFrame
data.drop("id", axis=1, inplace=True)

# Spécifier le nombre de clusters (K=5)
nombre_de_clusters = 5

# Créer une instance du modèle K-Means
kmeans = KMeans(n_clusters=nombre_de_clusters, random_state=0)

# Adapter le modèle aux données
kmeans.fit(data)

# Ajouter les étiquettes de cluster au DataFrame original
data["cluster"] = kmeans.labels_

# Réduire les dimensions des données à 2 composantes avec PCA
pca = PCA(n_components=2)
data_pca = pca.fit_transform(data.drop("cluster", axis=1))

# Ajouter les étiquettes de cluster à la version en 2D des données
data_pca = pd.DataFrame(data_pca, columns=["Dimension_1", "Dimension_2"])
data_pca["cluster"] = data["cluster"]

# Créer un nuage de points avec des couleurs pour chaque cluster
plt.figure(figsize=(10, 6))
colors = ['b', 'g', 'r', 'c', 'm']
for i in range(nombre_de_clusters):
    cluster_data = data_pca[data_pca['cluster'] == i]
    plt.scatter(cluster_data['Dimension_1'], cluster_data['Dimension_2'], c=colors[i], label=f'Cluster {i}')

plt.xlabel("Dimension_1")
plt.ylabel("Dimension_2")
plt.title(f"Représentation graphique des clusters (K={nombre_de_clusters})")
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Charger le fichier CSV contenant la colonne "cluster"
data = pd.read_csv("/content/drive/MyDrive/TPE_Mon_Travail/BD1-Cluster5_Title.csv")

# Compter le nombre d'occurrences de chaque cluster
cluster_counts = data['cluster'].value_counts().sort_index()

# Générer un graphique à barres empilées
plt.figure(figsize=(10, 6))
plt.bar(cluster_counts.index, cluster_counts.values, color='b')
plt.title('Histogramme des clusters (K=5)')
plt.xlabel('Cluster')
plt.ylabel('Nombre de documents')
plt.show()

"""**Regroupement des mots de chaque cluster pour l'analyse**

**Pour K=3**
"""

import pandas as pd
from collections import Counter

# Charger le fichier CSV contenant les clusters
input_path = r"/content/drive/MyDrive/TPE_Mon_Travail/BD1-Cluster3_Title.csv"
data = pd.read_csv(input_path)

# Créez un DataFrame pour stocker les mots clés par cluster
clusters_keywords = pd.DataFrame(columns=["Cluster", "Keywords"])

# Pour chaque cluster unique dans la colonne "cluster"
unique_clusters = data["cluster"].unique()
for cluster_id in unique_clusters:
    # Filtrez les données pour obtenir uniquement les lignes de ce cluster
    cluster_data = data[data["cluster"] == cluster_id]

    # Créez une liste vide pour stocker tous les mots
    all_words = []

    # Parcourez chaque ligne du cluster
    for index, row in cluster_data.iterrows():
        # Sélectionnez les noms des colonnes où la valeur est égale à 1, à l'exception de "cluster"
        words = [col for col in row.index if row[col] == 1 and col != "cluster"]
        all_words.extend(words)

    # Comptez la fréquence de chaque mot
    word_counts = Counter(all_words)

    # Triez les mots-clés par ordre alphabétique
    keywords = sorted(word_counts.keys())

    # Créez une chaîne de mots-clés
    keywords_string = ", ".join(keywords)

    # Ajoutez la paire (Cluster, Mots-clés) au DataFrame clusters_keywords
    clusters_keywords = clusters_keywords.append({"Cluster": cluster_id, "Keywords": keywords_string}, ignore_index=True)

# Enregistrez le DataFrame contenant les mots clés par cluster dans un fichier CSV
output_path = r"/content/drive/MyDrive/TPE_Mon_Travail/BD1-Cluster-Liste_Title.csv"
clusters_keywords.to_csv(output_path, index=False)

# Affichez le DataFrame contenant les mots-clés de chaque cluster
print(clusters_keywords)

"""**Pour K=4**"""

import pandas as pd
from collections import Counter

# Charger le fichier CSV contenant les clusters
input_path = r"/content/drive/MyDrive/TPE_Mon_Travail/BD1-Cluster4_Title.csv"
data = pd.read_csv(input_path)

# Créez un DataFrame pour stocker les mots clés par cluster
clusters_keywords = pd.DataFrame(columns=["Cluster", "Keywords"])

# Pour chaque cluster unique dans la colonne "cluster"
unique_clusters = data["cluster"].unique()
for cluster_id in unique_clusters:
    # Filtrez les données pour obtenir uniquement les lignes de ce cluster
    cluster_data = data[data["cluster"] == cluster_id]

    # Créez une liste vide pour stocker tous les mots
    all_words = []

    # Parcourez chaque ligne du cluster
    for index, row in cluster_data.iterrows():
        # Sélectionnez les noms des colonnes où la valeur est égale à 1, à l'exception de "cluster"
        words = [col for col in row.index if row[col] == 1 and col != "cluster"]
        all_words.extend(words)

    # Supprimez le mot "cluster" de la liste des mots-clés
    all_words = [word for word in all_words if word != "cluster"]

    # Comptez la fréquence de chaque mot
    word_counts = Counter(all_words)

    # Triez les mots-clés par ordre alphabétique
    keywords = sorted(word_counts.keys())

    # Créez une chaîne de mots-clés
    keywords_string = ", ".join(keywords)

    # Ajoutez la paire (Cluster, Mots-clés) au DataFrame clusters_keywords
    clusters_keywords = clusters_keywords.append({"Cluster": cluster_id, "Keywords": keywords_string}, ignore_index=True)

# Enregistrez le DataFrame contenant les mots clés par cluster dans un fichier CSV
output_path = r"/content/drive/MyDrive/TPE_Mon_Travail/BD1-Cluster4-Liste_Title.csv"
clusters_keywords.to_csv(output_path, index=False)

# Affichez le DataFrame contenant les mots-clés de chaque cluster
print(clusters_keywords)

"""**Pour K=5**"""

import pandas as pd
from collections import Counter

# Charger le fichier CSV contenant les clusters
input_path = r"/content/drive/MyDrive/TPE_Mon_Travail/BD1-Cluster5_Title.csv"
data = pd.read_csv(input_path)

# Créez un DataFrame pour stocker les mots clés par cluster
clusters_keywords = pd.DataFrame(columns=["Cluster", "Keywords"])

# Pour chaque cluster unique dans la colonne "cluster"
unique_clusters = data["cluster"].unique()
for cluster_id in unique_clusters:
    # Filtrez les données pour obtenir uniquement les lignes de ce cluster
    cluster_data = data[data["cluster"] == cluster_id]

    # Créez une liste vide pour stocker tous les mots
    all_words = []

    # Parcourez chaque ligne du cluster
    for index, row in cluster_data.iterrows():
        # Sélectionnez les noms des colonnes où la valeur est égale à 1, à l'exception de "cluster"
        words = [col for col in row.index if row[col] == 1 and col != "cluster"]
        all_words.extend(words)

    # Supprimez le mot "cluster" de la liste des mots-clés
    all_words = [word for word in all_words if word != "cluster"]

    # Comptez la fréquence de chaque mot
    word_counts = Counter(all_words)

    # Triez les mots-clés par ordre alphabétique
    keywords = sorted(word_counts.keys())

    # Créez une chaîne de mots-clés
    keywords_string = ", ".join(keywords)

    # Ajoutez la paire (Cluster, Mots-clés) au DataFrame clusters_keywords
    clusters_keywords = clusters_keywords.append({"Cluster": cluster_id, "Keywords": keywords_string}, ignore_index=True)

# Enregistrez le DataFrame contenant les mots clés par cluster dans un fichier CSV
output_path = r"/content/drive/MyDrive/TPE_Mon_Travail/BD1-Cluster5-Liste_Title.csv"
clusters_keywords.to_csv(output_path, index=False)

# Affichez le DataFrame contenant les mots-clés de chaque cluster
print(clusters_keywords)

"""**Extraction de tout le contenu**"""

!pip install pdfplumber

import os
import pdfplumber
import spacy
import pandas as pd

# Charger le modèle de langue SpaCy
nlp = spacy.load("en_core_web_sm")

# Expressions à supprimer
expressions_a_supprimer = [
    "RAPPORT","ACCIDENT","www","bea","aero","D","ENQUETE","SECURITE","BEA_Aero"
]

# Fonction pour filtrer les éléments non importants
def is_important(token):
    return not (token.is_stop or token.is_punct or token.pos_ in ['DET', 'PRON', 'PREP'])

# Fonction pour extraire le contenu et effectuer le traitement linguistique
def extract_text(pdf_path):
    with pdfplumber.open(pdf_path) as pdf:
        extracted_text = ""

        for page_num in range(len(pdf.pages)):
            page_text = pdf.pages[page_num].extract_text()
            extracted_text += page_text

        # Supprimer les expressions à supprimer
        for expression in expressions_a_supprimer:
            extracted_text = extracted_text.replace(expression, "")

        doc = nlp(extracted_text)
        important_tokens = [token.text for token in doc if is_important(token)]
        named_entities = [(ent.text, ent.label_) for ent in doc.ents]
        parsed_trees = [sent for sent in doc.sents]

        return extracted_text, important_tokens, named_entities, parsed_trees

# Liste pour stocker les données avant de les enregistrer dans un DataFrame
data_list = []

pdf_folder = "/content/drive/MyDrive/TPE_Mon_Travail/Rapport_BEA"
pdf_files = [os.path.join(pdf_folder, f) for f in os.listdir(pdf_folder) if f.endswith(".pdf")]

for pdf_file in pdf_files:
    extracted_text, important_tokens, named_entities, parsed_trees = extract_text(pdf_file)

    # Ajouter les données à la liste
    data_list.append({
        "id": pdf_file,
        "contenu": extracted_text
    })

# Créer un DataFrame pandas à partir de la liste de données
df = pd.DataFrame(data_list)

# Chemin vers le fichier CSV de sortie
output_file_path = "/content/drive/MyDrive/TPE_Mon_Travail/dataset_Contenu.csv"

# Enregistrer le DataFrame dans un fichier CSV
df.to_csv(output_file_path, index=False, encoding='utf-8')

print("Résultats enregistrés dans le fichier:", output_file_path)

"""**Nettoyage de l'Extraction de tous les contenus**"""

import pandas as pd
import re
from google.colab import drive

# Monter Google Drive pour accéder aux fichiers
drive.mount('/content/drive')

# Chemin vers le fichier CSV sur Google Drive
csv_file_path = "/content/drive/MyDrive/TPE_Mon_Travail/dataset_Contenu.csv"

# Lire le fichier CSV en utilisant pandas
data = pd.read_csv(csv_file_path, encoding='utf-8')

# Supprimer l'expression "Conséquences et dommages" de la colonne "contenu"
data['contenu'] = data['contenu'].str.replace('Conséquences et dommages', '')

# Liste des catégories d'éléments à supprimer
categories_a_supprimer = {
    "Articles définis": ["le", "la", "les", "l'"],
    "Prépositions et articles partitifs": ["de", "du", "des", "d'", "d", "au", "aux", "en"],
    "Articles indéfinis": ["un", "une"],
    "Prépositions": ["à", "dans", "sur", "pour", "par", "pas", "sans", "avec", "où"],
    "Conjonction de coordination": ["et", "car", "ou"],
    "Pronoms relatifs": ["que", "qui", "qu'"],
    "Pronoms personnels et réflexifs": ["il", "nous", "ce", "ces", "se", "ceci", "donc", "dont", "alors", "lors", "autre","en"]
}

# Fonction pour nettoyer le texte en supprimant les articles et les mots courts
def nettoyer_texte(texte, mots_a_supprimer):
    # Utiliser une expression régulière pour séparer les mots
    mots = re.findall(r'\b\w+\b', str(texte))

    # Filtrer les mots en supprimant les articles, les mots spécifiques
    # et les mots courts (un ou deux caractères)
    mots_filtrés = [mot for mot in mots if len(mot) > 2 and mot.lower() not in mots_a_supprimer]

    # Rejoindre les mots filtrés pour former le texte nettoyé
    texte_nettoyé = ' '.join(mots_filtrés)
    return texte_nettoyé

# Appliquer la fonction de nettoyage à la colonne de contenu
for categorie, mots_a_supprimer in categories_a_supprimer.items():
    data['contenu'] = data['contenu'].apply(lambda x: nettoyer_texte(x, mots_a_supprimer))

# Supprimer le chemin "/content/drive/MyDrive/TPE_Mon_Travail/Rapport_BEA" de la colonne 'id'
data['id'] = data['id'].str.split("/").str[-1]

# Enregistrer le résultat dans un nouveau fichier CSV sur Google Drive avec un encodage spécifique (par exemple, UTF-8)
cleaned_csv_file_path = "/content/drive/MyDrive/TPE_Mon_Travail/BD2_Contenu.csv"
data.to_csv(cleaned_csv_file_path, index=False, encoding='utf-8')

# Afficher les premières lignes du DataFrame nettoyé
print(data)

"""**Le Doc2Vec sur le contenu**"""

!pip install gensim

import nltk
nltk.download('punkt')

import pandas as pd
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
import multiprocessing

# Charger le DataFrame
input_path = r"/content/drive/MyDrive/TPE_Mon_Travail/BD2_Contenu.csv"
data = pd.read_csv(input_path)

# Tokenization : Divisez le texte en mots
data['tokens'] = data['contenu'].apply(lambda x: word_tokenize(x.lower()))

# Créez des documents étiquetés pour Doc2Vec
documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(data['tokens'])]

# Configuration de Doc2Vec
cores = multiprocessing.cpu_count()
model = Doc2Vec(vector_size=100,  # Taille des vecteurs de mots
                window=5,  # Fenêtre contextuelle
                min_count=1,  # Ignorer les mots avec une fréquence inférieure à 1
                workers=cores,  # Nombre de cœurs CPU à utiliser
                dm=1,  # Utiliser l'algorithme de "Distributed Memory"
                dbow_words=1)  # Entraîner des vecteurs de mots

# Construction du vocabulaire
model.build_vocab(documents)

# Entraînement du modèle Doc2Vec
model.train(documents, total_examples=model.corpus_count, epochs=10)

# Obtenir les vecteurs de documents
document_vectors = [model.infer_vector(doc.words) for doc in documents]

# Créez un DataFrame pour les vecteurs de documents
vectors_df = pd.DataFrame(document_vectors, columns=[f"d_{i}" for i in range(model.vector_size)])

# Créer un DataFrame avec la colonne "id"
id_df = data[['id']]

# Enregistrez le DataFrame résultant
output_path = r"/content/drive/MyDrive/TPE_Mon_Travail/BD2_Contenu-Doc2Vec.csv"
result_df = pd.concat([id_df, vectors_df], axis=1)
result_df.to_csv(output_path, index=False)

# Affichez le DataFrame résultant avec la colonne "id" et les vecteurs de documents
print(result_df)



"""**Fusion Titre et contenu**"""

import pandas as pd

# Chemins d'accès vers les fichiers CSV
path_bow = "/content/drive/MyDrive/TPE_Mon_Travail/BD1-Cluster3_Title.csv"
path_doc2vec = "/content/drive/MyDrive/TPE_Mon_Travail/BD2_Contenu-Doc2Vec.csv"

# Charger les deux DataFrames
df_bow = pd.read_csv(path_bow)
df_doc2vec = pd.read_csv(path_doc2vec)

# Fusionner les deux DataFrames en fonction de la colonne "id"
merged_df = df_bow.merge(df_doc2vec, on="id")

# Déplacer la colonne "cluster" à la fin du DataFrame fusionné
cluster_column = merged_df.pop("cluster")
merged_df["cluster"] = cluster_column

# Enregistrer le DataFrame fusionné dans un nouveau fichier CSV
output_path = "/content/drive/MyDrive/TPE_Mon_Travail/BD_Fusion.csv"
merged_df.to_csv(output_path, index=False)

# Afficher le DataFrame fusionné
print(merged_df)

df_bow = pd.read_csv(path_bow)
df_doc2vec = pd.read_csv(path_doc2vec)
df_cluster=df_bow.drop(columns=df_bow.columns[1:-1])
merged_df = df_cluster.merge(df_doc2vec, on="id")

# Déplacer la colonne "cluster" à la fin du DataFrame fusionné
cluster_column = merged_df.pop("cluster")
merged_df["cluster"] = cluster_column

# Enregistrer le DataFrame fusionné dans un nouveau fichier CSV
output_path = "/content/drive/MyDrive/TPE_Mon_Travail/BD_Fusion.csv"
merged_df.to_csv(output_path, index=False)

# Afficher le DataFrame fusionné
print(merged_df)

"""**Entrainement du modèle**"""

df_doc2vec

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report

# Chemin vers le fichier CSV (assurez-vous de spécifier le chemin correct)
#csv_file_path = "/content/drive/MyDrive/TPE_Mon_Travail/BD_Fusion.csv"

# Lire le fichier CSV en utilisant pandas
#data = pd.read_csv(csv_file_path, encoding='utf-8')
data=merged_df
print(data.shape)

# Séparer les caractéristiques (X) de la variable cible (y)
X = data.values[:,1:-1]
y = data["cluster"]
print(X.shape)
print(y.shape)

# Diviser les données en ensembles d'entraînement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Créer une instance du modèle KNeighborsClassifier (KNN) avec k=3
knn = KNeighborsClassifier(n_neighbors=3)

# Entraîner le modèle KNN sur l'ensemble d'entraînement
knn.fit(X_train, y_train)

# Faire des prédictions sur l'ensemble de test
y_pred = knn.predict(X_test)

# Calculer l'exactitude du modèle
accuracy = accuracy_score(y_test, y_pred)
print(f"Exactitude du modèle : {accuracy}")

# Afficher le rapport de classification
class_report = classification_report(y_test, y_pred)
print("Rapport de classification :\n", class_report)

# Sauvegarder les prédictions dans un fichier CSV
predictions_df = pd.DataFrame({'Vraie valeur': y_test, 'Prédiction': y_pred})
predictions_csv_path = "/content/drive/MyDrive/TPE_Mon_Travail/predictions_knn.csv"
predictions_df.to_csv(predictions_csv_path, index=False)

# Afficher les prédictions
print("Prédictions :\n", predictions_df)